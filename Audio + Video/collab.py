# -*- coding: utf-8 -*-
"""Untitled43.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xxB1Y_fCy7xJbyuAbFDGn4kJODFHN7mM
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
from sklearn.model_selection import train_test_split

# --- Load Data ---
# Update this path to your project folder in Google Drive
DRIVE_PATH = "/content/drive/MyDrive/Btech_Project/"

X_facial = np.load(DRIVE_PATH + 'X_facial.npy')
Y_facial = np.load(DRIVE_PATH + 'Y_facial_labels.npy')

X_audio = np.load(DRIVE_PATH + 'X_audio.npy')
Y_audio = np.load(DRIVE_PATH + 'Y_audio_labels.npy')

# --- CRITICAL: Verify Data Alignment ---
assert np.array_equal(Y_facial, Y_audio), "Label files are not identical! Data is misaligned."
print("Data verification successful. Labels are aligned.")

# Use one label file as the ground truth
Y = Y_facial

# --- Split Data into Training and Testing Sets (80% train, 20% test) ---
X_facial_train, X_facial_test, Y_train, Y_test = train_test_split(
    X_facial, Y, test_size=0.2, random_state=42, stratify=Y
)

X_audio_train, X_audio_test, _, _ = train_test_split(
    X_audio, Y, test_size=0.2, random_state=42, stratify=Y
)

print("Data shapes:")
print("Facial Train:", X_facial_train.shape, "Audio Train:", X_audio_train.shape, "Labels Train:", Y_train.shape)
print("Facial Test:", X_facial_test.shape, "Audio Test:", X_audio_test.shape, "Labels Test:", Y_test.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Bidirectional, LSTM, GRU, Dense, Dropout

def build_bilstm_model(input_shape):
    """Builds a Bi-directional LSTM model."""
    model = Sequential()

    # Input layer and Bi-directional LSTM
    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape))
    model.add(Bidirectional(LSTM(32))) # Second Bi-LSTM layer

    # Dropout for regularization
    model.add(Dropout(0.5))

    # Dense hidden layer
    model.add(Dense(32, activation='relu'))

    # Final output layer for binary classification
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Create the facial model using our builder function
facial_input_shape = (X_facial_train.shape[1], X_facial_train.shape[2])
facial_model = build_bilstm_model(facial_input_shape)

print("--- Training Facial Model ---")
facial_model.summary()

# Train the model
history_facial = facial_model.fit(
    X_facial_train, Y_train,
    validation_data=(X_facial_test, Y_test),
    epochs=50, # You can experiment with this number
    batch_size=32
)

# Reshape audio data for LSTM (add a 'timesteps' dimension)
# The audio data is (samples, features), we need (samples, timesteps, features)
X_audio_train = np.expand_dims(X_audio_train, axis=1)
X_audio_test = np.expand_dims(X_audio_test, axis=1)


# Create the audio model
audio_input_shape = (X_audio_train.shape[1], X_audio_train.shape[2])
audio_model = build_bilstm_model(audio_input_shape)

print("\n--- Training Audio Model ---")
audio_model.summary()

# Train the model
history_audio = audio_model.fit(
    X_audio_train, Y_train,
    validation_data=(X_audio_test, Y_test),
    epochs=50,
    batch_size=32
)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# --- Evaluate Facial Model ---
print("="*20)
print("Facial Model Evaluation")
print("="*20)
loss_facial, accuracy_facial = facial_model.evaluate(X_facial_test, Y_test, verbose=0)
print(f"Test Accuracy: {accuracy_facial*100:.2f}%")

# Get detailed report
Y_pred_facial_probs = facial_model.predict(X_facial_test)
Y_pred_facial = (Y_pred_facial_probs > 0.5).astype(int)
print("\nClassification Report:")
print(classification_report(Y_test, Y_pred_facial, target_names=['Not Stressed', 'Stressed']))


# --- Evaluate Audio Model ---
print("\n" + "="*20)
print("Audio Model Evaluation")
print("="*20)
loss_audio, accuracy_audio = audio_model.evaluate(X_audio_test, Y_test, verbose=0)
print(f"Test Accuracy: {accuracy_audio*100:.2f}%")

# Get detailed report
Y_pred_audio_probs = audio_model.predict(X_audio_test)
Y_pred_audio = (Y_pred_audio_probs > 0.5).astype(int)
print("\nClassification Report:")
print(classification_report(Y_test, Y_pred_audio, target_names=['Not Stressed', 'Stressed']))

# --- Implement Fusion ---
# We already have the probabilities from the step above (Y_pred_facial_probs, Y_pred_audio_probs)

# Average the probabilities from both models
Y_pred_fused_probs = (Y_pred_facial_probs + Y_pred_audio_probs) / 2.0

# Convert the fused probabilities into final class predictions (0 or 1)
Y_pred_fused = (Y_pred_fused_probs > 0.5).astype(int)

print("\nFusion processing complete.")

from sklearn.metrics import accuracy_score

# --- Evaluate Fused Model ---
print("\n" + "="*20)
print("Fused Multimodal Model Evaluation")
print("="*20)
accuracy_fused = accuracy_score(Y_test, Y_pred_fused)
print(f"Test Accuracy: {accuracy_fused*100:.2f}%")

# Get detailed report
print("\nClassification Report:")
print(classification_report(Y_test, Y_pred_fused, target_names=['Not Stressed', 'Stressed']))

# Display the confusion matrix
print("\nConfusion Matrix:")
cm = confusion_matrix(Y_test, Y_pred_fused)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Stressed', 'Stressed'], yticklabels=['Not Stressed', 'Stressed'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# --- Final Comparison ---
print("\n" + "="*30)
print("Final Model Performance Summary")
print("="*30)
print(f"Facial Model Accuracy: {accuracy_facial*100:.2f}%")
print(f"Audio Model Accuracy:  {accuracy_audio*100:.2f}%")
print(f"Fused Model Accuracy:  {accuracy_fused*100:.2f}%")
print("="*30)

"""## COMPARING DIFFERENT MODELS

"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Bidirectional, LSTM, GRU, Dense, Dropout, Conv1D, GlobalMaxPooling1D
)

# Model 1: Bi-LSTM (You already have this)
def build_bilstm_model(input_shape):
    model = Sequential([
        Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape),
        Bidirectional(LSTM(32)),
        Dropout(0.5),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Model 2: Bi-GRU
def build_bigru_model(input_shape):
    model = Sequential([
        Bidirectional(GRU(64, return_sequences=True), input_shape=input_shape),
        Bidirectional(GRU(32)),
        Dropout(0.5),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Model 3: 1D-CNN
def build_1dcnn_model(input_shape):
    # Adjust kernel size for timestep dimension
    kernel_size = 3 if input_shape[0] > 1 else 1
    model = Sequential([
        Conv1D(64, kernel_size=kernel_size, activation='relu', input_shape=input_shape),
        Conv1D(64, kernel_size=kernel_size, activation='relu'),
        GlobalMaxPooling1D(),
        Dropout(0.5),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

import pandas as pd

# Define the models you want to compare
models_to_compare = {
    "Bi-LSTM": build_bilstm_model,
    "Bi-GRU": build_bigru_model,
    "1D-CNN": build_1dcnn_model
}

# Store results
results = {}

# Get the input shape from your facial data
facial_input_shape = (X_facial_train.shape[1], X_facial_train.shape[2])

# Loop through each model
for model_name, build_fn in models_to_compare.items():
    print(f"\n--- Training {model_name} on Facial Data ---")

    # Build the model
    model = build_fn(facial_input_shape)

    # Train it
    history = model.fit(
        X_facial_train, Y_train,
        validation_data=(X_facial_test, Y_test),
        epochs=50,
        batch_size=32,
        verbose=0 # Set to 0 to keep the output clean
    )

    # Evaluate on the test set
    loss, accuracy = model.evaluate(X_facial_test, Y_test, verbose=0)

    # Save the results
    results[model_name] = {
        "Accuracy": accuracy,
        "Loss": loss,
        "Val_Accuracy": history.history['val_accuracy'][-1]
    }
    print(f"  - Test Accuracy: {accuracy*100:.2f}%")

# --- Display Final Results ---
results_df = pd.DataFrame(results).T
print("\n--- Facial Model Comparison Summary ---")
print(results_df)

# Add this function with your other model builders
def build_dnn_model(input_shape):
    """Builds a simple Dense Neural Network (MLP)."""
    model = Sequential([
        Dense(64, activation='relu', input_shape=input_shape),
        Dropout(0.5),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# ===================================================================
# 1. RELOAD AND PREPARE DATA FROM SOURCE (THE FIX)
# This guarantees our data starts with the correct 2D shape every time.
# ===================================================================
DRIVE_PATH = "/content/drive/MyDrive/Btech_Project/"
X_audio = np.load(DRIVE_PATH + 'X_audio.npy')
Y = np.load(DRIVE_PATH + 'Y_audio_labels.npy') # Use one consistent label file

# Split the original 2D data
X_audio_train, X_audio_test, Y_train, Y_test = train_test_split(
    X_audio, Y, test_size=0.2, random_state=42, stratify=Y
)

# Now, create the specific shapes needed for each model type
# For DNN: Use the 2D data directly
X_audio_train_dnn = X_audio_train
X_audio_test_dnn = X_audio_test

# For RNNs: Create the 3D data from the original 2D data
X_audio_train_rnn = np.expand_dims(X_audio_train, axis=1)
X_audio_test_rnn = np.expand_dims(X_audio_test, axis=1)

print(f"Shape for DNN models (2D): {X_audio_train_dnn.shape}")
print(f"Shape for RNN models (3D): {X_audio_train_rnn.shape}")

# ===================================================================
# 2. RUN MODEL COMPARISON
# ===================================================================

# Your model builder functions (build_bilstm_model, etc.) remain the same
models_to_compare = {
    "Bi-LSTM": build_bilstm_model,
    "Bi-GRU": build_bigru_model,
    "DNN": build_dnn_model
}

audio_results = {}

for model_name, build_fn in models_to_compare.items():
    print(f"\n--- Training {model_name} on Audio Data ---")

    # Use the correctly shaped data for each model
    if model_name == "DNN":
        X_train_data, X_test_data = X_audio_train_dnn, X_audio_test_dnn
        input_shape = (X_train_data.shape[1],)
    else: # For Bi-LSTM and Bi-GRU
        X_train_data, X_test_data = X_audio_train_rnn, X_audio_test_rnn
        input_shape = (X_train_data.shape[1], X_train_data.shape[2])

    model = build_fn(input_shape)

    history = model.fit(
        X_train_data, Y_train,
        validation_data=(X_test_data, Y_test),
        epochs=50, batch_size=32, verbose=0
    )

    loss, accuracy = model.evaluate(X_test_data, Y_test, verbose=0)
    audio_results[model_name] = {"Accuracy": accuracy, "Loss": loss}
    print(f"  - Test Accuracy: {accuracy*100:.2f}%")

# --- Display Final Audio Results ---
audio_results_df = pd.DataFrame(audio_results).T
print("\n--- Audio Model Comparison Summary ---")
print(audio_results_df)